{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks in Okareo: An Introduction\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/checks.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "After using this notebook, you will be able to:\n",
    "- Access the list of available `checks` in Okareo\n",
    "- Generate and upload a custom `check` to Okareo\n",
    "- Use `checks` to assess the behaviors of registered models in Okareo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the Okareo library and use your [API key](https://docs.okareo.com/docs/guides/environment#setting-up-your-okareo-environment) to authenticate. You will also need an [OpenAI API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = \"<YOUR_OKAREO_API_KEY>\"\n",
    "OPENAI_API_KEY = \"<YOUR_OPENAI_API_KEY>\"\n",
    "\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading a Scenario\n",
    "\n",
    "Here we use an existing `.jsonl` file to create a seed scenario with the `upload_scenario_set` method. The data here includes short questions about a fictitious company called \"WebBizz.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path_articles = \"webbizz_retrieval_questions.jsonl\"\n",
    "scenario_name_articles = \"WebBizz Retrieval Questions\"\n",
    "\n",
    "def load_or_download_file(file_path, scenario_name):\n",
    "    try:\n",
    "        # load the file to okareo\n",
    "        source_scenario = okareo.upload_scenario_set(file_path=file_path, scenario_name=scenario_name)\n",
    "    except:\n",
    "        print(f\"- Loading file {file_path} to Okareo failed. Temporarily download the file from GitHub...\") \n",
    "\n",
    "        # if the file doesn't exist, download it\n",
    "        file_url = f\"https://raw.githubusercontent.com/okareo-ai/okareo-python-sdk/main/examples/{file_path}\"\n",
    "        response = requests.get(file_url)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # load the file to okareo\n",
    "        source_scenario = okareo.upload_scenario_set(file_path=file_path, scenario_name=scenario_name)\n",
    "\n",
    "        # delete the file\n",
    "        os.remove(file_path)\n",
    "    return source_scenario\n",
    "\n",
    "source_scenario  = load_or_download_file(file_path_articles, scenario_name_articles)\n",
    "print(f\"{scenario_name_articles}: {source_scenario.app_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a Model\n",
    "\n",
    "For this notebook, we will register a simple model that makes the scenario `input` more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "from okareo.model_under_test import OpenAIModel\n",
    "\n",
    "random_string = ''.join(random.choices(string.ascii_letters, k=5))\n",
    "\n",
    "mut_name = f\"OpenAI Concise\"\n",
    "eval_name = f\"OpenAI Concise Test Run\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"{scenario_input}\"\n",
    "BREVITY_CONTEXT_TEMPLATE = \"\"\"\n",
    "Rewrite the following text in a more concise manner:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Registering model...\")\n",
    "# Register the model to use in the test run\n",
    "model_under_test = okareo.register_model(\n",
    "name=mut_name,\n",
    "model=OpenAIModel(\n",
    "    model_id=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    system_prompt_template=BREVITY_CONTEXT_TEMPLATE,\n",
    "    user_prompt_template=USER_PROMPT_TEMPLATE,\n",
    "),\n",
    ")\n",
    "\n",
    "print(f\"Model registered: {model_under_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-defined Checks\n",
    "\n",
    "To bootstrap your LLM evaluation workflow, Okareo offers pre-defined checks. Let's list the available checks with `okareo.get_all_checks()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checks = okareo.get_all_checks()\n",
    "all_checks_names = [check.name for check in all_checks]\n",
    "all_checks_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if our model is making the input more concise, let's use the `conciseness` and `levenshtein_distance_input` checks.\n",
    "\n",
    "We can get some more details on these by running the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = ['conciseness', 'levenshtein_distance_input']\n",
    "for check in all_checks:\n",
    "    if check.name in checks:\n",
    "        print(f\"--- {check.name} ---\")\n",
    "        print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checks above can be used when calling `run_test` on a model under test using the `checks` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation = model_under_test.run_test(\n",
    "    name=eval_name,\n",
    "    scenario=source_scenario,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    test_run_type=TestRunType.NL_GENERATION,\n",
    "    calculate_metrics=True,\n",
    "    checks=checks,\n",
    ")\n",
    "print(f\"See results in Okareo: {evaluation.app_link}\")\n",
    "metrics = evaluation.model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Custom Check\n",
    "\n",
    "In addition to the Okareo's predefined checks, you can use Okareo to generate custom checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo_api_client.models import EvaluatorSpecRequest\n",
    "\n",
    "description = (\n",
    "    \"Calculate the number of tokens in the model_output divided by the number of tokens in the scenario_input.\"\n",
    "    \"Get the number of tokens by doing a simple `len().split(' ')` call on model_output and scenario_input.\"\n",
    ")\n",
    "output_data_type = \"float\"\n",
    "\n",
    "generate_request = EvaluatorSpecRequest(\n",
    "    description=description,\n",
    "    requires_scenario_input=True,\n",
    "    requires_scenario_result=False,\n",
    "    output_data_type=output_data_type\n",
    ")\n",
    "generated_test = okareo.generate_check(generate_request).generated_code\n",
    "print(generated_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the check to Okareo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, add the check to your local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_name = 'token_compression_ratio'\n",
    "file_path = f\"{check_name}.py\"\n",
    "with open(file_path, \"w+\") as file:\n",
    "    file.write(generated_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, import the Check class and call Okareo's create_or_update_check method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_compression_ratio import Check\n",
    "\n",
    "token_cr_check = okareo.create_or_update_check(\n",
    "    name=check_name,\n",
    "    description=description,\n",
    "    check=Check()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the uploaded check\n",
    "\n",
    "Finally, we can use the uploaded check by adding it to the `checks` parameter of `run_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation with the custom check\n",
    "evaluation = model_under_test.run_test(\n",
    "    name=eval_name,\n",
    "    scenario=source_scenario,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    test_run_type=TestRunType.NL_GENERATION,\n",
    "    calculate_metrics=True,\n",
    "    checks=['conciseness', 'levenshtein_distance_input', check_name],\n",
    ")\n",
    "print(f\"See results in Okareo: {evaluation.app_link}\")\n",
    "metrics = evaluation.model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
