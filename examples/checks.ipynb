{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks in Okareo: An Introduction\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/checks.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "After using this notebook, you will be able to:\n",
    "- Access the list of available `checks` in Okareo\n",
    "- Generate and upload a custom `check` to Okareo\n",
    "- Use `checks` to assess the behaviors of registered models in Okareo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the Okareo library and use your [API key](https://docs.okareo.com/docs/guides/environment#setting-up-your-okareo-environment) to authenticate. You will also need an [OpenAI API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = \"<YOUR_OKAREO_API_KEY>\"\n",
    "OPENAI_API_KEY = \"<YOUR_OPENAI_API_KEY>\"\n",
    "\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading a Scenario\n",
    "\n",
    "Here we use an existing `.jsonl` file to create a seed scenario with the `upload_scenario_set` method. The data here includes short questions about a fictitious company called \"WebBizz.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path_articles = \"webbizz_retrieval_questions.jsonl\"\n",
    "scenario_name_articles = \"WebBizz Retrieval Questions\"\n",
    "\n",
    "def load_or_download_file(file_path, scenario_name):\n",
    "    try:\n",
    "        # load the file to okareo\n",
    "        source_scenario = okareo.upload_scenario_set(file_path=file_path, scenario_name=scenario_name)\n",
    "    except:\n",
    "        print(f\"- Loading file {file_path} to Okareo failed. Temporarily download the file from GitHub...\") \n",
    "\n",
    "        # if the file doesn't exist, download it\n",
    "        file_url = f\"https://raw.githubusercontent.com/okareo-ai/okareo-python-sdk/main/examples/{file_path}\"\n",
    "        response = requests.get(file_url)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # load the file to okareo\n",
    "        source_scenario = okareo.upload_scenario_set(file_path=file_path, scenario_name=scenario_name)\n",
    "\n",
    "        # delete the file\n",
    "        os.remove(file_path)\n",
    "    return source_scenario\n",
    "\n",
    "source_scenario  = load_or_download_file(file_path_articles, scenario_name_articles)\n",
    "print(f\"{scenario_name_articles}: {source_scenario.app_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a Model\n",
    "\n",
    "For this notebook, we will register a simple model that makes the scenario `input` more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "from okareo.model_under_test import OpenAIModel\n",
    "\n",
    "random_string = ''.join(random.choices(string.ascii_letters, k=5))\n",
    "\n",
    "mut_name = f\"OpenAI Concise\"\n",
    "eval_name = f\"OpenAI Concise Test Run\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"{input}\"\n",
    "BREVITY_CONTEXT_TEMPLATE = \"\"\"\n",
    "Rewrite the following text in a more concise manner:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Registering model...\")\n",
    "# Register the model to use in the test run\n",
    "model_under_test = okareo.register_model(\n",
    "name=mut_name,\n",
    "model=OpenAIModel(\n",
    "    model_id=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    system_prompt_template=BREVITY_CONTEXT_TEMPLATE,\n",
    "    user_prompt_template=USER_PROMPT_TEMPLATE,\n",
    "),\n",
    ")\n",
    "\n",
    "print(f\"Model registered: {model_under_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-defined Checks\n",
    "\n",
    "To bootstrap your LLM evaluation workflow, Okareo offers pre-defined checks. Let's list the available checks with `okareo.get_all_checks()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coherence_summary',\n",
       " 'consistency_summary',\n",
       " 'fluency_summary',\n",
       " 'relevance_summary',\n",
       " 'consistency',\n",
       " 'coherence',\n",
       " 'conciseness',\n",
       " 'fluency',\n",
       " 'uniqueness',\n",
       " 'levenshtein_distance',\n",
       " 'levenshtein_distance_input',\n",
       " 'compression_ratio',\n",
       " 'does_code_compile',\n",
       " 'contains_all_imports']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_checks = okareo.get_all_checks()\n",
    "all_checks_names = [check.name for check in all_checks]\n",
    "all_checks_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if our model is making the input more concise, let's use the `conciseness` and `levenshtein_distance_input` checks.\n",
    "\n",
    "We can get some more details on these by running the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- conciseness ---\n",
      "EvaluatorBriefResponse(id='14a5cf92-102a-47a0-9bfe-d26fd123d4f7', name='conciseness', description='A measure of economy of words in the model_output. Lower scores indicate unnecessary verbosity in the model_output. Ranges from 1 to 5.', output_data_type='float', time_created=datetime.datetime(2024, 4, 11, 11, 0), additional_properties={})\n",
      "--- levenshtein_distance_input ---\n",
      "EvaluatorBriefResponse(id='cfcff942-c11e-44ee-9660-7716f2b997e1', name='levenshtein_distance_input', description='Calculate the Levenshtein Distance between the model output and the scenario input.', output_data_type='int', time_created=datetime.datetime(2024, 4, 9, 12, 0), additional_properties={})\n"
     ]
    }
   ],
   "source": [
    "checks = ['conciseness', 'levenshtein_distance_input']\n",
    "for check in all_checks:\n",
    "    if check.name in checks:\n",
    "        print(f\"--- {check.name} ---\")\n",
    "        print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checks above can be used when calling `run_test` on a model under test using the `checks` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See results in Okareo: https://app.okareo.com/project/d38b3714-8c8f-4d69-8c07-cc7285bbe1b5/eval/69418399-888b-4bd9-aa04-558655657aeb\n"
     ]
    }
   ],
   "source": [
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation = model_under_test.run_test(\n",
    "    name=eval_name,\n",
    "    scenario=source_scenario,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    test_run_type=TestRunType.NL_GENERATION,\n",
    "    calculate_metrics=True,\n",
    "    checks=checks,\n",
    ")\n",
    "print(f\"See results in Okareo: {evaluation.app_link}\")\n",
    "metrics = evaluation.model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Custom Check\n",
    "\n",
    "In addition to the Okareo's predefined checks, you can use Okareo to generate custom checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from abc import ABC, abstractmethod\n",
      "import re\n",
      "import nltk\n",
      "import spacy\n",
      "import sklearn\n",
      "\n",
      "class BaseCheck(ABC):\n",
      "    @staticmethod\n",
      "    @abstractmethod\n",
      "    def evaluate():\n",
      "        pass\n",
      "\n",
      "class Check(BaseCheck):\n",
      "    @staticmethod\n",
      "    def evaluate(model_output: str, scenario_input: str) -> float:\n",
      "        # Calculate the number of tokens in model_output and scenario_input\n",
      "        model_output_tokens = len(model_output.split(' '))\n",
      "        scenario_input_tokens = len(scenario_input.split(' '))\n",
      "        \n",
      "        # Calculate the ratio of tokens in model_output to tokens in scenario_input\n",
      "        token_ratio = model_output_tokens / scenario_input_tokens\n",
      "        \n",
      "        return token_ratio\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from okareo_api_client.models import EvaluatorSpecRequest\n",
    "\n",
    "description = (\n",
    "    \"Calculate the number of tokens in the model_output divided by the number of tokens in the scenario_input.\"\n",
    "    \"Get the number of tokens by doing a simple `len().split(' ')` call on model_output and scenario_input.\"\n",
    ")\n",
    "output_data_type = \"float\"\n",
    "\n",
    "generate_request = EvaluatorSpecRequest(\n",
    "    description=description,\n",
    "    requires_scenario_input=True,\n",
    "    requires_scenario_result=False,\n",
    "    output_data_type=output_data_type\n",
    ")\n",
    "generated_test = okareo.generate_check(generate_request).generated_code\n",
    "print(generated_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the check to Okareo\n",
    "\n",
    "First, we save the generated code to a temp file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "check_name = 'token_compression_ratio'\n",
    "temp_dir = tempfile.gettempdir()\n",
    "file_path = os.path.join(temp_dir, f\"{check_name}.py\")\n",
    "with open(file_path, \"w+\") as file:\n",
    "    file.write(generated_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pass the `file_path` to the `upload_check` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cr_check = okareo.upload_check(\n",
    "    name=check_name,\n",
    "    file_path=file_path,\n",
    "    requires_scenario_input=True,\n",
    "    requires_scenario_result=False,\n",
    "    description=description,\n",
    "    output_data_type=output_data_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the uploaded check\n",
    "\n",
    "Finally, we can use the uploaded check by adding it to the `checks` parameter of `run_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See results in Okareo: https://app.okareo.com/project/d38b3714-8c8f-4d69-8c07-cc7285bbe1b5/eval/903db520-4fdc-4760-adc8-b9366c0ff52a\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation with the custom check\n",
    "evaluation = model_under_test.run_test(\n",
    "    name=eval_name,\n",
    "    scenario=source_scenario,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    test_run_type=TestRunType.NL_GENERATION,\n",
    "    calculate_metrics=True,\n",
    "    checks=['conciseness', 'levenshtein_distance_input', check_name],\n",
    ")\n",
    "print(f\"See results in Okareo: {evaluation.app_link}\")\n",
    "metrics = evaluation.model_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
