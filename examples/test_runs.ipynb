{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Zero Instructions:\n",
    "\n",
    "1. Install Okareo's Python SDK: &nbsp;&nbsp;  `pip install okareo`  &nbsp;&nbsp;  (just run the cell below)\n",
    "\n",
    "2. Get your API token from [https://app.okareo.com/](https://app.okareo.com/).  \n",
    "   (Note: You will need to register first.)\n",
    "\n",
    "3. Go directly to the **\"2. Create your API Token\"** link on the landing page in above app. You can skip all other steps.\n",
    "\n",
    "4. Set the environment variable `OKAREO_API_KEY` to your generated API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install okareo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8e4b89c5-4100-46c5-bcb8-a7884189ff32\n",
      "{'weighted_average': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.0}, 'scores_by_label': {'complains': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'complaints': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'returns': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'pricing': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}}\n"
     ]
    }
   ],
   "source": [
    "# perform a test run using a scenario set from one of the scenario_set notebook examples\n",
    "import os\n",
    "from okareo import Okareo\n",
    "from okareo.model_under_test import CustomModel\n",
    "from types import SimpleNamespace\n",
    "\n",
    "OKAREO_API_KEY = os.environ[\"OKAREO_API_KEY\"]\n",
    "okareo = Okareo(OKAREO_API_KEY)\n",
    "\n",
    "# Callable to be applied to each scenario in the scenario set\n",
    "def call_model(input: str):\n",
    "    # call your model being tested here using <input> from the scenario set\n",
    "\n",
    "    # mock code returnign a random label\n",
    "    labels = [\"returns\", \"complains\", \"pricing\"]\n",
    "    import random\n",
    "    actual = random.choice(labels)\n",
    "\n",
    "    return actual, {\"labels\": actual, \"confidence\": 0.8 }  # return a tuple of (actual, overall model response context)\n",
    "\n",
    "class RetrievalModel(CustomModel):\n",
    "    def invoke(self, input: str):\n",
    "        return call_model(input)\n",
    "\n",
    "# this will return a model if it already exists or create a new one if it doesn't\n",
    "model_under_test = okareo.register_model(name=\"intent_classifier\", model=RetrievalModel(name=\"custom classification\"))\n",
    "\n",
    "# use the scenario id from one of the scenario set notebook examples\n",
    "scenario={'scenario_id': '47cc9b5b-524b-4f7f-8a34-c1757be44ec0'}\n",
    "\n",
    "test_run_item = model_under_test.run_test(scenario=SimpleNamespace(**scenario), name=\"Intent Classifier Run 3\", calculate_metrics=True)\n",
    "\n",
    "# display model level metrics for the test run\n",
    "print(test_run_item.id)\n",
    "print(test_run_item.model_metrics.to_dict())\n",
    "print(test_run_item.app_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weighted_average': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.0}, 'scores_by_label': {'complains': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'complaints': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'returns': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'pricing': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve metrics from an earlier test run\n",
    "import os\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = os.environ[\"OKAREO_API_KEY\"]\n",
    "okareo = Okareo(OKAREO_API_KEY)\n",
    "\n",
    "# this will return a model if it already exists or create a new one if it doesn't\n",
    "model_under_test = okareo.register_model(name=\"intent_classifier\")\n",
    "#test run id from the previous cell output\n",
    "test_run_item = model_under_test.get_test_run(test_run_id=test_run_item.id)\n",
    "\n",
    "# display model level metrics for the test run\n",
    "print(test_run_item.model_metrics.to_dict())\n",
    "print(test_run_item.app_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
