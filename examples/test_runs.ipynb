{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Zero Instructions:\n",
    "\n",
    "1. Install Okareo's Python SDK: &nbsp;&nbsp;  `pip install okareo`  &nbsp;&nbsp;  (just run the cell below)\n",
    "\n",
    "2. Get your API token from [https://app.okareo.com/](https://app.okareo.com/).  \n",
    "   (Note: You will need to register first.)\n",
    "\n",
    "3. Go directly to the **\"2. Create your API Token\"** link on the landing page in above app. You can skip all other steps.\n",
    "\n",
    "4. Set the environment variable `OKAREO_API_KEY` to your generated API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install okareo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weighted_average': {'precision': 0.5, 'recall': 0.6666666666666666, 'f1': 0.5555555555555555, 'accuracy': 0.6666666666666666}, 'scores_by_label': {'pricing': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'complaints': {'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666}, 'returns': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}}}\n",
      "https://app.okareo.com/project/21b8a05b-f5b6-4578-a36a-fc264036d9d3/eval/d18fe542-2fb4-42c7-91a1-83cfeb83578d\n"
     ]
    }
   ],
   "source": [
    "# perform a test run using a scenario set\n",
    "import os\n",
    "from okareo import Okareo\n",
    "from okareo_api_client.models import ScenarioSetCreate, SeedData, ScenarioType\n",
    "from okareo.model_under_test import CustomModel\n",
    "from types import SimpleNamespace\n",
    "\n",
    "OKAREO_API_KEY = os.environ[\"OKAREO_API_KEY\"]\n",
    "okareo = Okareo(OKAREO_API_KEY)\n",
    "\n",
    "class RetrievalModel(CustomModel):\n",
    "    # Callable to be applied to each scenario in the scenario set\n",
    "    def invoke(self, input: str):\n",
    "        # call your model being tested here using <input> from the scenario set\n",
    "\n",
    "        # mock code returning a random label\n",
    "        labels = [\"returns\", \"complaints\", \"pricing\"]\n",
    "        import random\n",
    "        actual = random.choice(labels)\n",
    "\n",
    "        return actual, {\"labels\": actual, \"confidence\": 0.8 }  # return a tuple of (actual, overall model response context)\n",
    "\n",
    "# this will return a model if it already exists or create a new one if it doesn't\n",
    "model_under_test = okareo.register_model(name=\"intent_classifier\", model=RetrievalModel(name=\"custom classification\"))\n",
    "\n",
    "# generate example scenario based on seed data and return results in one call\n",
    "scenario_set_create = ScenarioSetCreate(name=\"My Test Scenario Set \",\n",
    "                                        generation_type=ScenarioType.SEED,\n",
    "                                        number_examples=1,\n",
    "                                        seed_data=[SeedData(input_=\"I want to send this product back\", result=\"returns\"),\n",
    "                                                   SeedData(input_=\"my product is not working\", result=\"complaints\"),\n",
    "                                                   SeedData(input_=\"how much is this product?\", result=\"pricing\")])\n",
    "\n",
    "source_scenario = okareo.create_scenario_set(scenario_set_create)\n",
    "scenario_id = source_scenario.scenario_id\n",
    "\n",
    "# use the scenario id to run the test\n",
    "test_run_item = model_under_test.run_test(scenario=scenario_id, name=\"Intent Classifier Run 3\", calculate_metrics=True)\n",
    "\n",
    "# display model level metrics for the test run\n",
    "print(test_run_item.model_metrics.to_dict())\n",
    "print(test_run_item.app_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weighted_average': {'precision': 0.5, 'recall': 0.6666666666666666, 'f1': 0.5555555555555555, 'accuracy': 0.6666666666666666}, 'scores_by_label': {'pricing': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'complaints': {'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666}, 'returns': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}}}\n",
      "https://app.okareo.com/project/21b8a05b-f5b6-4578-a36a-fc264036d9d3/eval/d18fe542-2fb4-42c7-91a1-83cfeb83578d\n"
     ]
    }
   ],
   "source": [
    "# Retrieve metrics from an earlier test run\n",
    "import os\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = os.environ[\"OKAREO_API_KEY\"]\n",
    "okareo = Okareo(OKAREO_API_KEY)\n",
    "\n",
    "# this will return a model if it already exists or create a new one if it doesn't\n",
    "model_under_test = okareo.register_model(name=\"intent_classifier\")\n",
    "#test run id from the previous cell output\n",
    "test_run_item = model_under_test.get_test_run(test_run_id=test_run_item.id)\n",
    "\n",
    "# display model level metrics for the test run\n",
    "print(test_run_item.model_metrics.to_dict())\n",
    "print(test_run_item.app_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
