{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/generation_eval.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## Welcome to Okareo!\n",
    "\n",
    "Get your API token from [https://app.okareo.com/](https://app.okareo.com/) and set it in the cell below. ðŸ‘‡\n",
    "   (Note: You will also need an OpenAI key.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKAREO_API_KEY = \"<YOUR-OKAREO-API-KEY>\"\n",
    "OPENAI_API_KEY = \"<YOUR-OPENAI-API-KEY>\"\n",
    "\n",
    "%pip install okareo openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to set up a simple generation task that will score a model on how well it can answer a question given some context. The questions will be about WebBizz, an example web business. The answer will be scored on how relevant it is for the given question. The setup will have 3 parts.\n",
    "\n",
    "1. Creating a scenario with questions and context.\n",
    "2. Setting up a generation model with prompts\n",
    "3. Adding a custom check and using it in an evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import tempfile\n",
    "from io import StringIO  \n",
    "import pandas as pd\n",
    "\n",
    "# Import Okareo libraries\n",
    "from okareo import Okareo\n",
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "from okareo_api_client.models import ScenarioSetCreate, SeedData\n",
    "\n",
    "# Create an instance of the Okareo client\n",
    "okareo = Okareo(OKAREO_API_KEY)\n",
    "\n",
    "# Load documents from Okareo's GitHub repository\n",
    "webbizz_articles = os.popen('curl https://raw.githubusercontent.com/okareo-ai/okareo-python-sdk/main/examples/webbizz_10_articles.jsonl').read()\n",
    "\n",
    "# Convert the JSONL string to a pandas DataFrame\n",
    "articlesJson = pd.read_json(path_or_buf=StringIO(webbizz_articles), lines=True)\n",
    "\n",
    "# Load questions from Okareo's GitHub repository\n",
    "webbizz_questions = os.popen('curl https://raw.githubusercontent.com/okareo-ai/okareo-python-sdk/main/examples/webbizz_retrieval_questions.jsonl').read()\n",
    "\n",
    "# Convert the JSONL string to a pandas DataFrame\n",
    "questionsJson = pd.read_json(path_or_buf=StringIO(webbizz_questions), lines=True)\n",
    "\n",
    "# Get the context for each question\n",
    "seed_inputs = questionsJson['input'].tolist()\n",
    "seed_contexts = []\n",
    "for i in range(len(seed_inputs)):\n",
    "    context = \"\"\n",
    "    # Get the context of the articles that are relevant to the question\n",
    "    for article_id in questionsJson['result'].tolist()[i]:\n",
    "        context += articlesJson[articlesJson['result'] == article_id]['input'].values[0] + \"\\n\"\n",
    "    seed_contexts.append(context)\n",
    "\n",
    "# Create a scenario set using the questions and contexts\n",
    "seed_data = []\n",
    "for i in range(len(seed_inputs)):\n",
    "    seed_data.append(SeedData(input_={'question': seed_inputs[i], 'context': seed_contexts[i]}, result='N/A'))\n",
    "scenario_set_create = ScenarioSetCreate(\n",
    "    name=f\"QA Scenario w/ Context- Webbizz\",\n",
    "    seed_data=seed_data\n",
    ")\n",
    "scenario = okareo.create_scenario_set(scenario_set_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answer Model\n",
    "We will be using GPT-4o from OpenAI to generate the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Okareo's handler for OpenAI models\n",
    "from okareo.model_under_test import OpenAIModel\n",
    "\n",
    "# Create an instance of the OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Define a template for the user prompt\n",
    "USER_PROMPT_TEMPLATE = \"Question: {input.question} Context: {input.context}\"\n",
    "\n",
    "# Define a template to prompt the model to provide an answer based on the context\n",
    "ANSWER_CONTEXT_TEMPLATE = \"\"\"\n",
    "You will be provided with context and a question.\n",
    "Answer the question based on the context.\n",
    "\"\"\"\n",
    "\n",
    "# Create an instance of the OpenAIModel class\n",
    "# This class is used to interact with the OpenAI model using user and system prompts\n",
    "openai_model = OpenAIModel(\n",
    "        model_id=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        system_prompt_template=ANSWER_CONTEXT_TEMPLATE,\n",
    "        user_prompt_template=USER_PROMPT_TEMPLATE,\n",
    "    )\n",
    "\n",
    "# Define the name of the model with the current timestamp\n",
    "mut_name=f\"OpenAI Answering Model - {datetime.now().strftime('%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Register the model to use in a test run\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=openai_model,\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom check with relevance prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.checks import ModelBasedCheck, CheckType\n",
    "\n",
    "# Create a relevance check for the QA scenario with context\n",
    "check = okareo.create_or_update_check(\n",
    "    name=\"Relevance Check\",\n",
    "    description=\"Relevance check for QA with context\",\n",
    "    check=ModelBasedCheck(\n",
    "        prompt_template=\"\"\"\n",
    "You will be given a question, context and answer.\n",
    "\n",
    "Your task is to rate the answer on one metric.\n",
    "\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Relevance (1-5) - selection of important content from the context. The answer should include only important information \n",
    "from the context that is relevant to the question. Annotators were instructed to penalize answers which contained\n",
    "redundancies and excess information.\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the question, context, and answer carefully.\n",
    "2. Compare the question to the context and identify the main points of the context.\n",
    "3. Assess how well the answer covers the information that the question is asking for.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\n",
    "Context:\n",
    "\n",
    "{input.context}\n",
    "\n",
    "Question:\n",
    "\n",
    "{input.question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "{model_output}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "The output should only be one number.\n",
    "- Relevance (1-5):\n",
    "        \"\"\",\n",
    "        check_type=CheckType.SCORE\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a name for the evaluation with the current timestamp\n",
    "eval_name = f\"QA Evaluation - {datetime.now().strftime('%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Perform a test run using the scenario set\n",
    "evaluation = model_under_test.run_test(\n",
    "    name=eval_name,\n",
    "    scenario=scenario,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    test_run_type=TestRunType.NL_GENERATION, # specify that we are testing a natural language generation model\n",
    "    calculate_metrics=True,\n",
    "    # Add the check we just created\n",
    "    checks=[check.name]\n",
    ")\n",
    "\n",
    "# Generate a link back to Okareo for evaluation visualization\n",
    "print(f\"See results in Okareo: {evaluation.app_link}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
