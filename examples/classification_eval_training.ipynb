{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/classification_eval_training.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## Welcome to Okareo!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install okareo transformers torch matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a model for classifying user queries into \"complaints,\" \"returns,\" or \"pricing.\"\n",
    "\n",
    "The trained model is used in the Classification Evaluation script:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/classification_eval.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained on data generated by an Okareo generator. Details on how the data was generated can be found in this notebook:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/generating_classification_scenarios.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data processing\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from Okareo's GitHub and convert the JSONL string to a pandas DataFrame\n",
    "data = pd.read_json(path_or_buf=\"https://raw.githubusercontent.com/okareo-ai/okareo-python-sdk/main/examples/webbizz_classification_questions.jsonl\", lines=True)\n",
    "\n",
    "# Convert the \"result\" column to numeric classes\n",
    "data[\"label\"] = data[\"result\"].map({\"complaints\": 2, \"returns\": 1, \"pricing\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a DistilBERT model and tokenizer\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries for PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data.sample(frac=0.8)\n",
    "val_data = data.drop(train_data.index)\n",
    "\n",
    "# Create a custom dataset class for the text data\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text and label for the given index\n",
    "        text = self.data.iloc[idx][\"input\"]\n",
    "        label = self.data.iloc[idx][\"label\"]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "        # Return the input IDs, attention mask, and label (reshape the input IDs and attention mask to remove an unneeded dimension)\n",
    "        return encoding.input_ids.squeeze(), encoding.attention_mask.squeeze(), label\n",
    "\n",
    "# Create Dataset objects for the training and validation sets\n",
    "train_data = TextDataset(train_data, tokenizer)\n",
    "val_data = TextDataset(val_data, tokenizer)\n",
    "\n",
    "# Create DataLoader objects for the training and validation sets\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An optimizer for the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# One epoch of training\n",
    "def train(loader, model, optimizer):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    print(\"Training...\")\n",
    "    losses = []\n",
    "    for input_ids, attention_mask, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # One-hot encode the labels\n",
    "        oh_labels = torch.nn.functional.one_hot(labels, num_classes=3).to(torch.float32)\n",
    "        # Pass the input IDs, attention mask, and one-hot labels to the model and get the loss\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=oh_labels).loss\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        # Track losses\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "# One epoch of validation\n",
    "def validate(loader, model, epoch):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    print(\"Validating...\")\n",
    "    # Disable gradient calculations (not needed for validation)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for input_ids, attention_mask, labels in loader:\n",
    "            # Pass the input IDs and attention mask to the model and get the logits\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            # Get the predicted labels\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            # Track accuracy\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch}, Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model for 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train the model for one epoch\n",
    "    losses = train(train_loader, model, optimizer)\n",
    "    # Plot the loss\n",
    "    plt.plot(losses)\n",
    "    plt.title(f\"Epoch {epoch} Loss\")\n",
    "    plt.show()\n",
    "    # Validate the model\n",
    "    validate(val_loader, model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model locally\n",
    "model.save_pretrained(\"webbizz_classification_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing to Hugging Face\n",
    "\n",
    "To make the model publically accessible, Okareo also saved the model on the Hugging Face Hub. After authenticating (not shown), the code below is how the model was published there.\n",
    "\n",
    "If you train your own model, you can use the local version by omitting `okareo/` (as shown below) when loading the model in your script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model from the local directory\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"webbizz_classification_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model to Hugging Face Hub\n",
    "# model.push_to_hub(\"webbizz_classification_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
