{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Finetuned Classification Model in Okareo\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/scenarios.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "After using this notebook, you will be able to:\n",
    "- Finetune an open source LLM for classification\n",
    "- Evaluate the finetuned model in Okareo\n",
    "- Generate synthetic data from misclassified points to augment the finetuning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = os.environ.get('OKAREO_API_KEY')\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import load_raw_data\n",
    "\n",
    "# load the train/test data\n",
    "file_paths = [\"./finetuning/webbizz_finetuning_train_data.jsonl\",\n",
    "              \"./finetuning/webbizz_finetuning_test_data.jsonl\"]\n",
    "data_dict = load_raw_data(file_paths)\n",
    "train_data = data_dict[file_paths[0]]\n",
    "test_data = data_dict[file_paths[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The name for a scenario must be unique. The scenario name WebBizz Tool Classification - train is already in use, and has been returned from this call.\n",
      "train: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/scenario/bed059c5-90c9-4908-a0f5-330271a1e9e3\n",
      "Warning: The name for a scenario must be unique. The scenario name WebBizz Tool Classification - test is already in use, and has been returned from this call.\n",
      "test: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/scenario/a41761a3-5c38-42ea-8ea7-2eccb6024aa1\n"
     ]
    }
   ],
   "source": [
    "from okareo_api_client.models.seed_data import SeedData\n",
    "from okareo_api_client.models import ScenarioSetCreate\n",
    "\n",
    "# create scenarios with full json results\n",
    "scenario_ids = {}\n",
    "for name, data in zip(['train', 'test'], [train_data, test_data]):\n",
    "    scenario_set_create = ScenarioSetCreate(\n",
    "        name=f\"WebBizz Tool Classification - {name}\",\n",
    "        seed_data=[SeedData(input_=d['input'], result=d['result']) for d in data] \n",
    "    )\n",
    "\n",
    "    source_scenario_static = okareo.create_scenario_set(scenario_set_create)\n",
    "    scenario_ids[name] = source_scenario_static.scenario_id\n",
    "\n",
    "    print(f'{name}: {source_scenario_static.app_link}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning instructions should be formatted with the following three fields:\n",
    "- Instruction: Description of the task, input/output format.\n",
    "- Input: Text used to prompt the LLM\n",
    "- Output: Expected response from the LLM\n",
    "\n",
    "We provide a template for our WebBizz classification task in 'finetuning.utils' file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
      "\n",
      "- pricing\n",
      "- complaints\n",
      "- returns\n",
      "\n",
      "Return only one category that is most relevant to the question.\n",
      "\n",
      "### Input:\n",
      "{input}\n",
      " \n",
      "### Output:\n",
      "{result}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import format_instruction_for_scenario\n",
    "\n",
    "post_template = format_instruction_for_scenario(\"{input}\")\n",
    "print(post_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_instruction_data = [post_template.format(input=d['input'], result=d['result']) for d in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
      "\n",
      "- pricing\n",
      "- complaints\n",
      "- returns\n",
      "\n",
      "Return only one category that is most relevant to the question.\n",
      "\n",
      "### Input:\n",
      "Can I send this product back?\n",
      " \n",
      "### Output:\n",
      "returns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(finetuning_instruction_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = f\"./finetuning/webbizz_finetuning_train_instructions.jsonl\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    for row in finetuning_instruction_data:\n",
    "        file.write(json.dumps({'sample': row}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mason/git/okareo-python-sdk/examples/finetuning/webbizz_finetuning_train_instructions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 24 examples [00:00, 4804.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "p = [os.getcwd(), \"finetuning\", \"webbizz_finetuning_train_instructions.jsonl\" ] \n",
    "filepath = os.path.join(*p)\n",
    "print(filepath)\n",
    "dataset = load_dataset('json', data_files={'train': file_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample': '### Instruction:\\nGiven \"Input\", return a category under \"Output\" that is one of the following:\\n\\n- pricing\\n- complaints\\n- returns\\n\\nReturn only one category that is most relevant to the question.\\n\\n### Input:\\nCan I send this product back?\\n \\n### Output:\\nreturns\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Phi-3 for finetuning\n",
    "\n",
    "Now we set up a finetuning run using the finetuning instruction scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_API_KEY=os.environ.get(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.66s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing with max_seq_length=94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/transformers/training_args.py:1960: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import get_model_tokenizer_trainer\n",
    " \n",
    "# Microsoft's huggingface model id for Phi-3\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# directory where finetuned model weights will be written\n",
    "finetuned_model_name = \"Phi-3-mini-4k-int4\"\n",
    "\n",
    "# target modules for LoRA\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# set up peft model/tokenizer/trainer for finetuning\n",
    "peft_model, tokenizer, trainer = get_model_tokenizer_trainer(model_id, finetuned_model_name, dataset, HUGGINGFACE_API_KEY, target_modules=target_modules, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 53.5696, 'train_samples_per_second': 1.12, 'train_steps_per_second': 0.112, 'train_loss': 1.2629125118255615, 'epoch': 2.4}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    " \n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the finetuned model in Okareo\n",
    "\n",
    "Now we will register the finetuned model in Okareo to perform classification evaluations on the train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model + unpatching flash attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.64s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import get_finetuned_model_tokenizer\n",
    "\n",
    "# load the peft model with the base model\n",
    "finetuned_model_name = \"Phi-3-mini-4k-int4\"\n",
    "peft_model, tokenizer = get_finetuned_model_tokenizer(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finetuning.utils import count_tokens\n",
    "\n",
    "# get the number of tokens in each expected output json\n",
    "n_train_tokens = [count_tokens(tokenizer, s) for s in train_data]\n",
    "\n",
    "# get limit of tokens to generate as closest 100s above the max\n",
    "nearest = 100\n",
    "n_token_lim = nearest * (max(n_train_tokens) // nearest) + nearest\n",
    "n_token_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.model_under_test import CustomModel, ModelInvocation\n",
    "from finetuning.utils import format_instruction\n",
    "\n",
    "mut_name = f\"WebBizz Tool Classification - Phi-3-mini-4k finetuned\"\n",
    "\n",
    "class FinetunedPhi3Model(CustomModel):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        self.model = peft_model\n",
    "        self.categories = [\"pricing\", \"complaints\", \"returns\"]\n",
    "\n",
    "    def invoke(self, input_value):\n",
    "        prompt = format_instruction({ \"question\": input_value })\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=n_token_lim,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        pred = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):].strip()\n",
    "        res = \"unknown\"\n",
    "        for cat in self.categories:\n",
    "            if cat in pred:\n",
    "                res = cat\n",
    "        return ModelInvocation(\n",
    "            actual=res,\n",
    "            model_input=input_value,\n",
    "            model_result=pred,\n",
    "        )\n",
    "\n",
    "# Register the model to use in the test run\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[FinetunedPhi3Model(name=FinetunedPhi3Model.__name__)],\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: See results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/54a9e820-8261-493f-a9ed-3ddf81722769\n",
      "test split: See results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/18b70061-a54c-4733-87f8-33a9eac62235\n"
     ]
    }
   ],
   "source": [
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "\n",
    "for name in [\"train\", \"test\"]:\n",
    "    eval_name = f\"Tool Classification ({name}, no synthetic data)\"\n",
    "    evaluation = model_under_test.run_test(\n",
    "        name=eval_name,\n",
    "        scenario=scenario_ids[name],\n",
    "        test_run_type=TestRunType.MULTI_CLASS_CLASSIFICATION,\n",
    "        calculate_metrics=True,\n",
    "    )\n",
    "    print(f\"{name} split: See results in Okareo: {evaluation.app_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating more finetuning data from failure cases\n",
    "\n",
    "To improve the finetuned model's performance, we will generate synthetic data from the mis-classified rows in the \"train\" split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo_api_client.api.default import (\n",
    "    find_test_data_points_v0_find_test_data_points_post,\n",
    ")\n",
    "\n",
    "# use the id from the train split evaluation run\n",
    "train_eval_id = \"54a9e820-8261-493f-a9ed-3ddf81722769\"\n",
    "\n",
    "# get evaluation data\n",
    "eval_response = model_under_test.get_test_run(train_eval_id)\n",
    "\n",
    "# get scenario data points\n",
    "scenario_rows = okareo.get_scenario_data_points(eval_response.scenario_set_id)\n",
    "\n",
    "scenario_inputs = {}\n",
    "scenario_results = {}\n",
    "for row in scenario_rows:\n",
    "    scenario_inputs[row.id] = row.input_\n",
    "    scenario_results[row.id] = row.result\n",
    "\n",
    "# get test data points\n",
    "tdp = find_test_data_points_v0_find_test_data_points_post.sync(\n",
    "    client=okareo.client,\n",
    "    json_body=find_test_data_points_v0_find_test_data_points_post.FindTestDataPointPayload(\n",
    "        test_run_id=train_eval_id,\n",
    "    ),\n",
    "    api_key=OKAREO_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile scenario and test data points with relevant scores\n",
    "scenario_to_test_dp = {}\n",
    "for dp in tdp:\n",
    "    scenario_to_test_dp[dp.scenario_data_point_id] = {\n",
    "        'input': scenario_inputs[dp.scenario_data_point_id],\n",
    "        'expected': dp.metric_value.additional_properties['expected'],\n",
    "        'actual': dp.metric_value.additional_properties['actual'],\n",
    "        'test_data_point_id': dp.id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the datapoints based on misclassified results\n",
    "filtered_dp = [dp for dp in scenario_to_test_dp.values() if dp['expected'] != dp['actual']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# write the failed rows\n",
    "file_path = \"./finetuning/webbizz_finetuning_train_failed_rows.jsonl\" \n",
    "with open(file_path, \"w\") as f:\n",
    "    for dp in filtered_dp:\n",
    "        json.dump({ \"input\": dp['input'], \"result\": dp['expected'] }, f)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_scenario_name = f\"WebBizz Tool Classification - train failures\"\n",
    "seed_scenario = okareo.upload_scenario_set(file_path=file_path, scenario_name=seed_scenario_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the same template from before, but this time we set it up for use with our scenario generator's `post_template` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
      "\n",
      "- pricing\n",
      "- complaints\n",
      "- returns\n",
      "\n",
      "Return only one category that is most relevant to the question.\n",
      "\n",
      "### Input:\n",
      "{generation.input}\n",
      " \n",
      "### Output:\n",
      "{result}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# format the scenario generation template to use the generator\n",
    "\n",
    "post_template_rephrase = format_instruction_for_scenario(\"{generation.input}\")\n",
    "print(post_template_rephrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo_api_client.models.generation_tone import GenerationTone\n",
    "from okareo_api_client.models import ScenarioSetGenerate, ScenarioType\n",
    "\n",
    "# generate rephrased versions \n",
    "generate_scenario_name = f\"{seed_scenario_name} rephrased\"\n",
    "\n",
    "generate_request = ScenarioSetGenerate(\n",
    "    source_scenario_id=seed_scenario.scenario_id,\n",
    "    name=generate_scenario_name,\n",
    "    number_examples=3,\n",
    "    generation_type=ScenarioType.REPHRASE_INVARIANT,\n",
    "    generation_tone=GenerationTone.NEUTRAL,\n",
    "    post_template=post_template_rephrase\n",
    ")\n",
    "\n",
    "rephrased_instruction_scenario = okareo.generate_scenario_set(generate_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scenario_id='bed059c5-90c9-4908-a0f5-330271a1e9e3'\n",
      "test_scenario_id='a41761a3-5c38-42ea-8ea7-2eccb6024aa1'\n",
      "rephrased_instruction_scenario_id='16b578f7-2ae3-48c1-a20b-85c2a1de2c97'\n"
     ]
    }
   ],
   "source": [
    "# copy the output of this cell into part 2 to continue finetuning with Okareo synthetic data\n",
    "print(f\"train_scenario_id='{scenario_ids['train']}'\")\n",
    "print(f\"test_scenario_id='{scenario_ids['test']}'\")\n",
    "print(f\"rephrased_instruction_scenario_id='{rephrased_instruction_scenario.scenario_id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
