{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Finetuning with Synthetic Data in Okareo\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/classification_finetuning_eval_part2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "Note: This notebook is a companion notebook to ['classification_finetuning_eval_part1.ipynb]. Please run all cells in that notebook before starting this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy-paste these IDs from the last cell of Part 1\n",
    "train_scenario_id='bed059c5-90c9-4908-a0f5-330271a1e9e3'\n",
    "test_scenario_id='a41761a3-5c38-42ea-8ea7-2eccb6024aa1'\n",
    "rephrased_instruction_scenario_id='16b578f7-2ae3-48c1-a20b-85c2a1de2c97'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_ids = {\n",
    "    'train': train_scenario_id,\n",
    "    'test': test_scenario_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = os.environ.get('OKAREO_API_KEY')\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import load_raw_data\n",
    "\n",
    "# load the train/test data\n",
    "file_paths = [\"./finetuning/webbizz_finetuning_train_data.jsonl\",\n",
    "              \"./finetuning/webbizz_finetuning_test_data.jsonl\"]\n",
    "data_dict = load_raw_data(file_paths)\n",
    "train_data = data_dict[file_paths[0]]\n",
    "test_data = data_dict[file_paths[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning instructions should be formatted with the following three fields:\n",
    "- Instruction: Description of the task, input/output format.\n",
    "- Input: Text used to prompt the LLM\n",
    "- Output: Expected response from the LLM\n",
    "\n",
    "We provide a template for our WebBizz classification task in 'finetuning.utils' file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
      "\n",
      "- pricing\n",
      "- complaints\n",
      "- returns\n",
      "\n",
      "Return only one category that is most relevant to the question.\n",
      "\n",
      "### Input:\n",
      "{input}\n",
      " \n",
      "### Output:\n",
      "{result}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import format_instruction_for_scenario\n",
    "\n",
    "post_template = format_instruction_for_scenario(\"{input}\")\n",
    "print(post_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_instruction_data = [post_template.format(input=d['input'], result=d['result']) for d in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
      "\n",
      "- pricing\n",
      "- complaints\n",
      "- returns\n",
      "\n",
      "Return only one category that is most relevant to the question.\n",
      "\n",
      "### Input:\n",
      "Can I send this product back?\n",
      " \n",
      "### Output:\n",
      "returns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(finetuning_instruction_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rephrased failure rows\n",
    "sdp = okareo.get_scenario_data_points(rephrased_instruction_scenario_id)\n",
    "for sd in sdp:\n",
    "    finetuning_instruction_data.append(sd.input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = f\"./finetuning/webbizz_finetuning_train_instructions_augmented.jsonl\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    for row in finetuning_instruction_data:\n",
    "        file.write(json.dumps({'sample': row}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mason/git/okareo-python-sdk/examples/finetuning/webbizz_finetuning_train_instructions_augmented.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 30 examples [00:00, 6367.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "p = [os.getcwd(), \"finetuning\", \"webbizz_finetuning_train_instructions_augmented.jsonl\" ] \n",
    "filepath = os.path.join(*p)\n",
    "print(filepath)\n",
    "dataset = load_dataset('json', data_files={'train': file_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Phi-3 for finetuning\n",
    "\n",
    "Now we set up a finetuning run using the finetuning instruction scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_API_KEY=os.environ.get(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:24<00:00, 12.01s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing with max_seq_length=95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/transformers/training_args.py:1960: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Generating train split: 25 examples [00:00, 1189.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import get_model_tokenizer_trainer\n",
    " \n",
    "# Microsoft's huggingface model id for Phi-3\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# directory where finetuned model weights will be written\n",
    "finetuned_model_name = \"Phi-3-mini-4k-int4-augmented\"\n",
    "\n",
    "# target modules for LoRA\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# set up peft model/tokenizer/trainer for finetuning\n",
    "peft_model, tokenizer, trainer = get_model_tokenizer_trainer(model_id, finetuned_model_name, dataset, HUGGINGFACE_API_KEY, target_modules=target_modules, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 54.4602, 'train_samples_per_second': 1.377, 'train_steps_per_second': 0.165, 'train_loss': 0.9350931379530165, 'epoch': 2.571428571428571}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    " \n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the finetuned model in Okareo\n",
    "\n",
    "Now we will register the finetuned model in Okareo to perform classification evaluations on the train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model + unpatching flash attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:30<00:00, 15.48s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import get_finetuned_model_tokenizer\n",
    "\n",
    "# load the peft model with the base model\n",
    "finetuned_model_name = \"Phi-3-mini-4k-int4-augmented\"\n",
    "peft_model, tokenizer = get_finetuned_model_tokenizer(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finetuning.utils import count_tokens\n",
    "\n",
    "# get the number of tokens in each expected output json\n",
    "n_train_tokens = [count_tokens(tokenizer, s) for s in train_data]\n",
    "\n",
    "# get limit of tokens to generate as closest 100s above the max\n",
    "nearest = 100\n",
    "n_token_lim = nearest * (max(n_train_tokens) // nearest) + nearest\n",
    "n_token_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.model_under_test import CustomModel, ModelInvocation\n",
    "from finetuning.utils import format_instruction\n",
    "\n",
    "mut_name = f\"WebBizz Tool Classification - Phi-3-mini-4k finetuned + augmented\"\n",
    "\n",
    "class FinetunedPhi3Model(CustomModel):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        self.model = peft_model\n",
    "        self.categories = [\"pricing\", \"complaints\", \"returns\"]\n",
    "\n",
    "    def invoke(self, input_value):\n",
    "        prompt = format_instruction({ \"question\": input_value })\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=n_token_lim,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        pred = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):].strip()\n",
    "        res = \"unknown\"\n",
    "        for cat in self.categories:\n",
    "            if cat in pred:\n",
    "                res = cat\n",
    "        return ModelInvocation(\n",
    "            actual=res,\n",
    "            model_input=input_value,\n",
    "            model_result=pred,\n",
    "        )\n",
    "\n",
    "# Register the model to use in the test run\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[FinetunedPhi3Model(name=FinetunedPhi3Model.__name__)],\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: See results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/63ef7e22-75c0-49e1-981f-25b8d2d76fa5\n",
      "test split: See results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/fd664f3e-6ace-4a82-8113-1b1d0bcf156c\n"
     ]
    }
   ],
   "source": [
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "\n",
    "for name in [\"train\", \"test\"]:\n",
    "    eval_name = f\"Tool Classification ({name}, with synthetic data)\"\n",
    "    evaluation = model_under_test.run_test(\n",
    "        name=eval_name,\n",
    "        scenario=scenario_ids[name],\n",
    "        test_run_type=TestRunType.MULTI_CLASS_CLASSIFICATION,\n",
    "        calculate_metrics=True,\n",
    "    )\n",
    "    print(f\"{name} split: See results in Okareo: {evaluation.app_link}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
