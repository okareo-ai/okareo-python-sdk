{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Intent Detection Performance with Synthetic Data in Okareo\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/okareo-ai/okareo-python-sdk/blob/main/examples/classification_finetuning_eval_part2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Note: This notebook is a companion notebook to [Part #1](https://github.com/okareo-ai/okareo-python-sdk/blob/main/examples/classification_finetuning_eval_part1.ipynb). Before proceeding, please ensure you have completed that notebook and cleared your CUDA cache (i.e., restarted that notebook) before starting this one.\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "After using this notebook, you will be able to:\n",
    "- Finetune an LLM with an augmented train split that includes synthetic data\n",
    "- Evaluate the newly finetuned model in Okareo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Finetuning Data from Part \\#1\n",
    "\n",
    "In this part, we use the same train and test splits as before. To get more finetuning data, we use the synthetic data generated from failed rows of the train split evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy-paste these IDs from the last cell of Part 1\n",
    "train_scenario_id='785f4786-e07b-473f-a417-395dbbbcfeb8'\n",
    "test_scenario_id='b679a2c0-72eb-4c84-9d05-0c2d268b86e3'\n",
    "rephrased_instruction_scenario_id='483521c5-4896-4356-86ab-e52412bbde97'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_ids = {\n",
    "    'train': train_scenario_id,\n",
    "    'test': test_scenario_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from okareo import Okareo\n",
    "\n",
    "OKAREO_API_KEY = os.environ.get('OKAREO_API_KEY')\n",
    "okareo = Okareo(OKAREO_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get training data from part #1\n",
    "sdp = okareo.get_scenario_data_points(train_scenario_id)\n",
    "data = {'input': [], 'label': []}\n",
    "for sd in sdp:\n",
    "    data['input'].append(sd.input_)\n",
    "    data['label'].append(sd.result)\n",
    "\n",
    "train_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same prompt template/formatting as in Part #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_PREAMBLE = \"\"\"### Instruction:\n",
    "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
    "\n",
    "- Newsletter\n",
    "- Miscellaneous\n",
    "- Sustainability\n",
    "- Membership\n",
    "- Support\n",
    "- Safety\n",
    "- Returns\n",
    "\n",
    "Return only one category that is most relevant to the question.\n",
    "\"\"\"\n",
    "\n",
    "def format_instruction_for_scenario(input_name=\"{input}\"):\n",
    "\treturn f\"\"\"{PROMPT_PREAMBLE}\n",
    "### Input:\n",
    "{input_name}\n",
    " \n",
    "### Output:\n",
    "{{result}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
      "\n",
      "- Newsletter\n",
      "- Miscellaneous\n",
      "- Sustainability\n",
      "- Membership\n",
      "- Support\n",
      "- Safety\n",
      "- Returns\n",
      "\n",
      "Return only one category that is most relevant to the question.\n",
      "\n",
      "### Input:\n",
      "{input}\n",
      " \n",
      "### Output:\n",
      "{result}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "post_template = format_instruction_for_scenario(\"{input}\")\n",
    "print(post_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_instruction_data = [\n",
    "    post_template.format(\n",
    "        input=train_df.loc[i, 'input'],\n",
    "        result=train_df.loc[i, 'label']\n",
    "    ) for i in range(train_df.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Given \"Input\", return a category under \"Output\" that is one of the following:\n",
      "\n",
      "- Newsletter\n",
      "- Miscellaneous\n",
      "- Sustainability\n",
      "- Membership\n",
      "- Support\n",
      "- Safety\n",
      "- Returns\n",
      "\n",
      "Return only one category that is most relevant to the question.\n",
      "\n",
      "### Input:\n",
      "What steps does WebBizz take to keep my personal and financial info safe?\n",
      " \n",
      "### Output:\n",
      "Safety\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(finetuning_instruction_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rephrased failure rows\n",
    "sdp = okareo.get_scenario_data_points(rephrased_instruction_scenario_id)\n",
    "for sd in sdp:\n",
    "    finetuning_instruction_data.append(sd.input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = f\"./finetuning/webbizz_finetuning_train_instructions_augmented.jsonl\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    for row in finetuning_instruction_data:\n",
    "        file.write(json.dumps({'sample': row}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mason/git/okareo-python-sdk/examples/finetuning/webbizz_finetuning_train_instructions_augmented.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 39 examples [00:00, 9307.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "p = [os.getcwd(), \"finetuning\", \"webbizz_finetuning_train_instructions_augmented.jsonl\" ] \n",
    "filepath = os.path.join(*p)\n",
    "print(filepath)\n",
    "dataset = load_dataset('json', data_files={'train': file_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Phi-3 on the Augmented Dataset\n",
    "\n",
    "Now we set up a finetuning run identical to the Part #1 run using the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_API_KEY=os.environ.get(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.31s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing with max_seq_length=126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/transformers/training_args.py:1960: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import get_model_tokenizer_trainer\n",
    " \n",
    "# Microsoft's huggingface model id for Phi-3\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# directory where finetuned model weights will be written\n",
    "finetuned_model_name = \"Phi-3-mini-4k-int4-augmented\"\n",
    "\n",
    "# target modules for LoRA\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# set up peft model/tokenizer/trainer for finetuning\n",
    "peft_model, tokenizer, trainer = get_model_tokenizer_trainer(\n",
    "    model_id,\n",
    "    finetuned_model_name,\n",
    "    dataset,\n",
    "    HUGGINGFACE_API_KEY,\n",
    "    target_modules=target_modules,\n",
    "    epochs=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9136, 'grad_norm': 0.44787299633026123, 'learning_rate': 0.0005, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/mason/miniconda3/envs/phi3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1173, 'grad_norm': 0.28994619846343994, 'learning_rate': 0.0005, 'epoch': 5.0}\n",
      "{'train_runtime': 113.8966, 'train_samples_per_second': 1.405, 'train_steps_per_second': 0.176, 'train_loss': 0.5154589891433716, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    " \n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the augmented finetuned model in Okareo\n",
    "\n",
    "As before, we register the augmented finetuned model in Okareo to perform the same classification evaluations. This will let us compare model performance pre-/post-augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model + unpatching flash attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.89s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from finetuning.utils import get_finetuned_model_tokenizer\n",
    "\n",
    "# load the peft model with the base model\n",
    "finetuned_model_name = \"Phi-3-mini-4k-int4-augmented\"\n",
    "peft_model, tokenizer = get_finetuned_model_tokenizer(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from okareo.model_under_test import CustomModel, ModelInvocation\n",
    "\n",
    "def format_instruction(sample):\n",
    "\tprompt = f\"\"\"{PROMPT_PREAMBLE}\n",
    "### Input:\n",
    "{sample['question']}\n",
    " \n",
    "### Output:\n",
    "\"\"\"\n",
    "\tif 'category' in sample.keys():\n",
    "\t\tprompt += f\"{sample['category']}\\n\"\n",
    "\treturn prompt\n",
    "\n",
    "mut_name = f\"WebBizz Intent Detection - Phi-3-mini-4k finetuned + augmented\"\n",
    "\n",
    "class FinetunedPhi3Model(CustomModel):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        self.model = peft_model\n",
    "        self.categories = [\n",
    "            \"Newsletter\",\n",
    "            \"Miscellaneous\",\n",
    "            \"Sustainability\",\n",
    "            \"Membership\",\n",
    "            \"Support\",\n",
    "            \"Safety\",\n",
    "            \"Returns\",\n",
    "        ]\n",
    "\n",
    "    def invoke(self, input_value):\n",
    "        prompt = format_instruction({ \"question\": input_value })\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=5,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        pred = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):].strip()\n",
    "        pred = pred.split(\"\\n\")[0]\n",
    "        res = \"Unknown\"\n",
    "        for cat in self.categories:\n",
    "            if cat in pred:\n",
    "                res = cat\n",
    "        return ModelInvocation(\n",
    "            actual=res,\n",
    "            model_input=input_value,\n",
    "            model_result=pred,\n",
    "        )\n",
    "\n",
    "# Register the model to use in the test run\n",
    "model_under_test = okareo.register_model(\n",
    "    name=mut_name,\n",
    "    model=[FinetunedPhi3Model(name=FinetunedPhi3Model.__name__)],\n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: See results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/f27cc179-51f7-44a8-b2e1-77b0a595ac68\n",
      "test split: See results in Okareo: https://app.okareo.com/project/89920a9a-54cc-40c8-af68-9975e64e8d18/eval/561543ac-a95b-4fa7-9240-12ad2a39646b\n"
     ]
    }
   ],
   "source": [
    "from okareo_api_client.models.test_run_type import TestRunType\n",
    "\n",
    "for name in [\"train\", \"test\"]:\n",
    "    eval_name = f\"Intent Detection ({name}, with synthetic data)\"\n",
    "    evaluation = model_under_test.run_test(\n",
    "        name=eval_name,\n",
    "        scenario=scenario_ids[name],\n",
    "        test_run_type=TestRunType.MULTI_CLASS_CLASSIFICATION,\n",
    "        calculate_metrics=True,\n",
    "    )\n",
    "    print(f\"{name} split: See results in Okareo: {evaluation.app_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
